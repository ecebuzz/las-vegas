hive> insert overwrite table q9_product_type_profit
    > select 
    >   nation, o_year, sum(amount) as sum_profit
    > from 
    >   (
    > select 
    >   n_name as nation, year(o_orderdate) as o_year, 
    >   l_extendedprice * (1 - l_discount) -  ps_supplycost * l_quantity as amount
    >     from
    >       orders o join
    >       (select l_extendedprice, l_discount, l_quantity, l_orderkey, n_name, ps_supplycost 
    >        from part p join
    >          (select l_extendedprice, l_discount, l_quantity, l_partkey, l_orderkey, 
    >                  n_name, ps_supplycost 
    >           from partsupp ps join
    >             (select l_suppkey, l_extendedprice, l_discount, l_quantity, l_partkey, 
    >                     l_orderkey, n_name 
    >              from
    >                (select s_suppkey, n_name 
    >                 from nation n join supplier s on n.n_nationkey = s.s_nationkey
    >                ) s1 join lineitem l on s1.s_suppkey = l.l_suppkey
    >             ) l1 on ps.ps_suppkey = l1.l_suppkey and ps.ps_partkey = l1.l_partkey
    >          ) l2 on p.p_name like '%green%' and p.p_partkey = l2.l_partkey
    >      ) l3 on o.o_orderkey = l3.l_orderkey
    >   )profit
    > group by nation, o_year
    > order by nation, o_year desc;
Total MapReduce jobs = 7
Launching Job 1 out of 7
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201111041621_0004, Tracking URL = http://poseidon.smn.cs.brown.edu:50030/jobdetails.jsp?jobid=job_201111041621_0004
Kill Command = /home/hkimura/hadoop-0.20.203.0/bin/../bin/hadoop job  -Dmapred.job.tracker=poseidon.smn.cs.brown.edu:9001 -kill job_201111041621_0004
2011-11-04 16:24:43,286 Stage-7 map = 0%,  reduce = 0%
2011-11-04 16:24:49,317 Stage-7 map = 100%,  reduce = 0%
2011-11-04 16:24:58,373 Stage-7 map = 100%,  reduce = 33%
2011-11-04 16:25:01,395 Stage-7 map = 100%,  reduce = 100%
Ended Job = job_201111041621_0004
Launching Job 2 out of 7
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201111041621_0005, Tracking URL = http://poseidon.smn.cs.brown.edu:50030/jobdetails.jsp?jobid=job_201111041621_0005
Kill Command = /home/hkimura/hadoop-0.20.203.0/bin/../bin/hadoop job  -Dmapred.job.tracker=poseidon.smn.cs.brown.edu:9001 -kill job_201111041621_0005
2011-11-04 16:25:16,957 Stage-8 map = 0%,  reduce = 0%
2011-11-04 16:25:23,004 Stage-8 map = 33%,  reduce = 0%
2011-11-04 16:25:26,034 Stage-8 map = 44%,  reduce = 0%
2011-11-04 16:25:29,050 Stage-8 map = 50%,  reduce = 0%
2011-11-04 16:25:32,068 Stage-8 map = 60%,  reduce = 11%
2011-11-04 16:25:35,114 Stage-8 map = 71%,  reduce = 11%
2011-11-04 16:25:38,132 Stage-8 map = 76%,  reduce = 11%
2011-11-04 16:25:41,150 Stage-8 map = 79%,  reduce = 11%
2011-11-04 16:25:44,168 Stage-8 map = 82%,  reduce = 11%
2011-11-04 16:25:47,194 Stage-8 map = 85%,  reduce = 11%
2011-11-04 16:25:49,207 Stage-8 map = 88%,  reduce = 22%
2011-11-04 16:25:52,225 Stage-8 map = 91%,  reduce = 22%
2011-11-04 16:25:55,242 Stage-8 map = 94%,  reduce = 22%
2011-11-04 16:25:58,260 Stage-8 map = 98%,  reduce = 22%
2011-11-04 16:26:01,278 Stage-8 map = 100%,  reduce = 22%
2011-11-04 16:26:25,403 Stage-8 map = 100%,  reduce = 67%
2011-11-04 16:26:28,422 Stage-8 map = 100%,  reduce = 70%
2011-11-04 16:26:31,439 Stage-8 map = 100%,  reduce = 73%
2011-11-04 16:26:34,456 Stage-8 map = 100%,  reduce = 77%
2011-11-04 16:26:37,474 Stage-8 map = 100%,  reduce = 80%
2011-11-04 16:26:40,490 Stage-8 map = 100%,  reduce = 83%
2011-11-04 16:26:43,508 Stage-8 map = 100%,  reduce = 87%
2011-11-04 16:26:46,524 Stage-8 map = 100%,  reduce = 90%
2011-11-04 16:26:49,540 Stage-8 map = 100%,  reduce = 93%
2011-11-04 16:26:52,557 Stage-8 map = 100%,  reduce = 96%
2011-11-04 16:26:55,576 Stage-8 map = 100%,  reduce = 100%
Ended Job = job_201111041621_0005
Launching Job 3 out of 7
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201111041621_0006, Tracking URL = http://poseidon.smn.cs.brown.edu:50030/jobdetails.jsp?jobid=job_201111041621_0006
Kill Command = /home/hkimura/hadoop-0.20.203.0/bin/../bin/hadoop job  -Dmapred.job.tracker=poseidon.smn.cs.brown.edu:9001 -kill job_201111041621_0006
2011-11-04 16:27:14,138 Stage-1 map = 0%,  reduce = 0%
2011-11-04 16:27:23,197 Stage-1 map = 45%,  reduce = 0%
2011-11-04 16:27:26,224 Stage-1 map = 57%,  reduce = 0%
2011-11-04 16:27:29,238 Stage-1 map = 59%,  reduce = 0%
2011-11-04 16:27:32,252 Stage-1 map = 62%,  reduce = 0%
2011-11-04 16:27:35,266 Stage-1 map = 65%,  reduce = 17%
2011-11-04 16:27:38,286 Stage-1 map = 68%,  reduce = 17%
2011-11-04 16:27:40,296 Stage-1 map = 71%,  reduce = 17%
2011-11-04 16:27:43,309 Stage-1 map = 74%,  reduce = 17%
2011-11-04 16:27:46,324 Stage-1 map = 77%,  reduce = 17%
2011-11-04 16:27:49,336 Stage-1 map = 80%,  reduce = 17%
2011-11-04 16:27:52,349 Stage-1 map = 83%,  reduce = 17%
2011-11-04 16:27:55,362 Stage-1 map = 86%,  reduce = 17%
2011-11-04 16:27:58,376 Stage-1 map = 89%,  reduce = 17%
2011-11-04 16:28:01,389 Stage-1 map = 92%,  reduce = 17%
2011-11-04 16:28:04,401 Stage-1 map = 95%,  reduce = 17%
2011-11-04 16:28:07,416 Stage-1 map = 98%,  reduce = 17%
2011-11-04 16:28:10,430 Stage-1 map = 100%,  reduce = 17%
2011-11-04 16:28:40,562 Stage-1 map = 100%,  reduce = 33%
2011-11-04 16:28:43,575 Stage-1 map = 100%,  reduce = 68%
2011-11-04 16:28:46,588 Stage-1 map = 100%,  reduce = 71%
2011-11-04 16:28:49,600 Stage-1 map = 100%,  reduce = 74%
2011-11-04 16:28:54,164 Stage-1 map = 100%,  reduce = 77%
2011-11-04 16:28:56,173 Stage-1 map = 100%,  reduce = 80%
2011-11-04 16:28:59,186 Stage-1 map = 100%,  reduce = 83%
2011-11-04 16:29:02,200 Stage-1 map = 100%,  reduce = 86%
2011-11-04 16:29:05,213 Stage-1 map = 100%,  reduce = 89%
2011-11-04 16:29:08,247 Stage-1 map = 100%,  reduce = 91%
2011-11-04 16:29:11,260 Stage-1 map = 100%,  reduce = 94%
2011-11-04 16:29:13,529 Stage-1 map = 100%,  reduce = 97%
2011-11-04 16:29:16,542 Stage-1 map = 100%,  reduce = 99%
2011-11-04 16:29:19,556 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201111041621_0006
Launching Job 4 out of 7
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201111041621_0007, Tracking URL = http://poseidon.smn.cs.brown.edu:50030/jobdetails.jsp?jobid=job_201111041621_0007
Kill Command = /home/hkimura/hadoop-0.20.203.0/bin/../bin/hadoop job  -Dmapred.job.tracker=poseidon.smn.cs.brown.edu:9001 -kill job_201111041621_0007
2011-11-04 16:29:35,105 Stage-2 map = 0%,  reduce = 0%
2011-11-04 16:29:41,137 Stage-2 map = 50%,  reduce = 0%
2011-11-04 16:29:44,150 Stage-2 map = 54%,  reduce = 0%
2011-11-04 16:29:47,162 Stage-2 map = 57%,  reduce = 0%
2011-11-04 16:29:50,175 Stage-2 map = 60%,  reduce = 17%
2011-11-04 16:29:53,188 Stage-2 map = 63%,  reduce = 17%
2011-11-04 16:29:56,200 Stage-2 map = 66%,  reduce = 17%
2011-11-04 16:29:59,212 Stage-2 map = 69%,  reduce = 17%
2011-11-04 16:30:02,224 Stage-2 map = 72%,  reduce = 17%
2011-11-04 16:30:05,236 Stage-2 map = 75%,  reduce = 17%
2011-11-04 16:30:08,249 Stage-2 map = 78%,  reduce = 17%
2011-11-04 16:30:11,261 Stage-2 map = 81%,  reduce = 17%
2011-11-04 16:30:14,275 Stage-2 map = 84%,  reduce = 17%
2011-11-04 16:30:17,795 Stage-2 map = 87%,  reduce = 17%
2011-11-04 16:30:19,804 Stage-2 map = 90%,  reduce = 17%
2011-11-04 16:30:22,817 Stage-2 map = 93%,  reduce = 17%
2011-11-04 16:30:25,871 Stage-2 map = 96%,  reduce = 17%
2011-11-04 16:30:28,883 Stage-2 map = 99%,  reduce = 17%
2011-11-04 16:30:31,896 Stage-2 map = 100%,  reduce = 17%
2011-11-04 16:31:05,029 Stage-2 map = 100%,  reduce = 67%
2011-11-04 16:31:08,044 Stage-2 map = 100%,  reduce = 72%
2011-11-04 16:31:11,056 Stage-2 map = 100%,  reduce = 78%
2011-11-04 16:31:14,070 Stage-2 map = 100%,  reduce = 84%
2011-11-04 16:31:17,083 Stage-2 map = 100%,  reduce = 90%
2011-11-04 16:31:20,097 Stage-2 map = 100%,  reduce = 95%
2011-11-04 16:31:23,111 Stage-2 map = 100%,  reduce = 100%
Ended Job = job_201111041621_0007
Launching Job 5 out of 7
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201111041621_0008, Tracking URL = http://poseidon.smn.cs.brown.edu:50030/jobdetails.jsp?jobid=job_201111041621_0008
Kill Command = /home/hkimura/hadoop-0.20.203.0/bin/../bin/hadoop job  -Dmapred.job.tracker=poseidon.smn.cs.brown.edu:9001 -kill job_201111041621_0008
2011-11-04 16:31:37,680 Stage-3 map = 0%,  reduce = 0%
2011-11-04 16:31:46,721 Stage-3 map = 79%,  reduce = 0%
2011-11-04 16:31:49,737 Stage-3 map = 99%,  reduce = 0%
2011-11-04 16:31:52,751 Stage-3 map = 100%,  reduce = 0%
2011-11-04 16:31:55,763 Stage-3 map = 100%,  reduce = 17%
2011-11-04 16:32:04,804 Stage-3 map = 100%,  reduce = 75%
2011-11-04 16:32:07,817 Stage-3 map = 100%,  reduce = 94%
2011-11-04 16:32:10,833 Stage-3 map = 100%,  reduce = 100%
Ended Job = job_201111041621_0008
Launching Job 6 out of 7
Number of reduce tasks not specified. Estimated from input data size: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201111041621_0009, Tracking URL = http://poseidon.smn.cs.brown.edu:50030/jobdetails.jsp?jobid=job_201111041621_0009
Kill Command = /home/hkimura/hadoop-0.20.203.0/bin/../bin/hadoop job  -Dmapred.job.tracker=poseidon.smn.cs.brown.edu:9001 -kill job_201111041621_0009
2011-11-04 16:32:26,234 Stage-4 map = 0%,  reduce = 0%
2011-11-04 16:32:32,254 Stage-4 map = 100%,  reduce = 0%
2011-11-04 16:32:44,297 Stage-4 map = 100%,  reduce = 100%
Ended Job = job_201111041621_0009
Launching Job 7 out of 7
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201111041621_0010, Tracking URL = http://poseidon.smn.cs.brown.edu:50030/jobdetails.jsp?jobid=job_201111041621_0010
Kill Command = /home/hkimura/hadoop-0.20.203.0/bin/../bin/hadoop job  -Dmapred.job.tracker=poseidon.smn.cs.brown.edu:9001 -kill job_201111041621_0010
2011-11-04 16:33:08,065 Stage-5 map = 0%,  reduce = 0%
2011-11-04 16:33:14,088 Stage-5 map = 100%,  reduce = 0%
2011-11-04 16:33:23,121 Stage-5 map = 100%,  reduce = 33%
2011-11-04 16:33:26,135 Stage-5 map = 100%,  reduce = 100%
Ended Job = job_201111041621_0010
Loading data to table default.q9_product_type_profit
Deleted hdfs://poseidon.smn.cs.brown.edu:9000/media/datavol/hive/warehouse/q9_product_type_profit
Table default.q9_product_type_profit stats: [num_partitions: 0, num_files: 1, num_rows: 0, total_size: 5819]
175 Rows loaded to q9_product_type_profit
OK
Time taken: 539.31 seconds



hive> select count(*) from part where p_name like '%green%';
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201111041621_0011, Tracking URL = http://poseidon.smn.cs.brown.edu:50030/jobdetails.jsp?jobid=job_201111041621_0011
Kill Command = /home/hkimura/hadoop-0.20.203.0/bin/../bin/hadoop job  -Dmapred.job.tracker=poseidon.smn.cs.brown.edu:9001 -kill job_201111041621_0011
2011-11-04 16:41:02,622 Stage-1 map = 0%,  reduce = 0%
2011-11-04 16:41:08,644 Stage-1 map = 100%,  reduce = 0%
2011-11-04 16:41:20,697 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201111041621_0011
OK
11637
Time taken: 33.695 seconds

hive> select count(*) from part;                            
Total MapReduce jobs = 1
Launching Job 1 out of 1
Number of reduce tasks determined at compile time: 1
In order to change the average load for a reducer (in bytes):
  set hive.exec.reducers.bytes.per.reducer=<number>
In order to limit the maximum number of reducers:
  set hive.exec.reducers.max=<number>
In order to set a constant number of reducers:
  set mapred.reduce.tasks=<number>
Starting Job = job_201111041621_0012, Tracking URL = http://poseidon.smn.cs.brown.edu:50030/jobdetails.jsp?jobid=job_201111041621_0012
Kill Command = /home/hkimura/hadoop-0.20.203.0/bin/../bin/hadoop job  -Dmapred.job.tracker=poseidon.smn.cs.brown.edu:9001 -kill job_201111041621_0012
2011-11-04 16:41:56,113 Stage-1 map = 0%,  reduce = 0%
2011-11-04 16:42:02,133 Stage-1 map = 100%,  reduce = 0%
2011-11-04 16:42:14,173 Stage-1 map = 100%,  reduce = 100%
Ended Job = job_201111041621_0012
OK
200000
Time taken: 33.589 seconds

OK
ABSTRACT SYNTAX TREE:
  (TOK_QUERY (TOK_FROM (TOK_SUBQUERY (TOK_QUERY (TOK_FROM (TOK_JOIN (TOK_TABREF (TOK_TABNAME orders) o) (TOK_SUBQUERY (TOK_QUERY (TOK_FROM (TOK_JOIN (TOK_TABREF (TOK_TABNAME part) p) (TOK_SUBQUERY (TOK_QUERY (TOK_FROM (TOK_JOIN (TOK_TABREF (TOK_TABNAME partsupp) ps) (TOK_SUBQUERY (TOK_QUERY (TOK_FROM (TOK_JOIN (TOK_SUBQUERY (TOK_QUERY (TOK_FROM (TOK_JOIN (TOK_TABREF (TOK_TABNAME nation) n) (TOK_TABREF (TOK_TABNAME supplier) s) (= (. (TOK_TABLE_OR_COL n) n_nationkey) (. (TOK_TABLE_OR_COL s) s_nationkey)))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_TABLE_OR_COL s_suppkey)) (TOK_SELEXPR (TOK_TABLE_OR_COL n_name))))) s1) (TOK_TABREF (TOK_TABNAME lineitem) l) (= (. (TOK_TABLE_OR_COL s1) s_suppkey) (. (TOK_TABLE_OR_COL l) l_suppkey)))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_TABLE_OR_COL l_suppkey)) (TOK_SELEXPR (TOK_TABLE_OR_COL l_extendedprice)) (TOK_SELEXPR (TOK_TABLE_OR_COL l_discount)) (TOK_SELEXPR (TOK_TABLE_OR_COL l_quantity)) (TOK_SELEXPR (TOK_TABLE_OR_COL l_partkey)) (TOK_SELEXPR (TOK_TABLE_OR_COL l_orderkey)) (TOK_SELEXPR (TOK_TABLE_OR_COL n_name))))) l1) (and (= (. (TOK_TABLE_OR_COL ps) ps_suppkey) (. (TOK_TABLE_OR_COL l1) l_suppkey)) (= (. (TOK_TABLE_OR_COL ps) ps_partkey) (. (TOK_TABLE_OR_COL l1) l_partkey))))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_TABLE_OR_COL l_extendedprice)) (TOK_SELEXPR (TOK_TABLE_OR_COL l_discount)) (TOK_SELEXPR (TOK_TABLE_OR_COL l_quantity)) (TOK_SELEXPR (TOK_TABLE_OR_COL l_partkey)) (TOK_SELEXPR (TOK_TABLE_OR_COL l_orderkey)) (TOK_SELEXPR (TOK_TABLE_OR_COL n_name)) (TOK_SELEXPR (TOK_TABLE_OR_COL ps_supplycost))))) l2) (and (like (. (TOK_TABLE_OR_COL p) p_name) '%green%') (= (. (TOK_TABLE_OR_COL p) p_partkey) (. (TOK_TABLE_OR_COL l2) l_partkey))))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_TABLE_OR_COL l_extendedprice)) (TOK_SELEXPR (TOK_TABLE_OR_COL l_discount)) (TOK_SELEXPR (TOK_TABLE_OR_COL l_quantity)) (TOK_SELEXPR (TOK_TABLE_OR_COL l_orderkey)) (TOK_SELEXPR (TOK_TABLE_OR_COL n_name)) (TOK_SELEXPR (TOK_TABLE_OR_COL ps_supplycost))))) l3) (= (. (TOK_TABLE_OR_COL o) o_orderkey) (. (TOK_TABLE_OR_COL l3) l_orderkey)))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (TOK_TABLE_OR_COL n_name) nation) (TOK_SELEXPR (TOK_FUNCTION year (TOK_TABLE_OR_COL o_orderdate)) o_year) (TOK_SELEXPR (- (* (TOK_TABLE_OR_COL l_extendedprice) (- 1 (TOK_TABLE_OR_COL l_discount))) (* (TOK_TABLE_OR_COL ps_supplycost) (TOK_TABLE_OR_COL l_quantity))) amount)))) profit)) (TOK_INSERT (TOK_DESTINATION (TOK_TAB (TOK_TABNAME q9_product_type_profit))) (TOK_SELECT (TOK_SELEXPR (TOK_TABLE_OR_COL nation)) (TOK_SELEXPR (TOK_TABLE_OR_COL o_year)) (TOK_SELEXPR (TOK_FUNCTION sum (TOK_TABLE_OR_COL amount)) sum_profit)) (TOK_GROUPBY (TOK_TABLE_OR_COL nation) (TOK_TABLE_OR_COL o_year)) (TOK_ORDERBY (TOK_TABSORTCOLNAMEASC (TOK_TABLE_OR_COL nation)) (TOK_TABSORTCOLNAMEDESC (TOK_TABLE_OR_COL o_year)))))

STAGE DEPENDENCIES:
  Stage-7 is a root stage
  Stage-8 depends on stages: Stage-7
  Stage-1 depends on stages: Stage-8
  Stage-2 depends on stages: Stage-1
  Stage-3 depends on stages: Stage-2
  Stage-4 depends on stages: Stage-3
  Stage-5 depends on stages: Stage-4
  Stage-0 depends on stages: Stage-5
  Stage-6 depends on stages: Stage-0

STAGE PLANS:
  Stage: Stage-7
    Map Reduce
      Alias -> Map Operator Tree:
        profit:l3:l2:l1:s1:n 
          TableScan
            alias: n
            Reduce Output Operator
              key expressions:
                    expr: n_nationkey
                    type: int
              sort order: +
              Map-reduce partition columns:
                    expr: n_nationkey
                    type: int
              tag: 0
              value expressions:
                    expr: n_name
                    type: string
        profit:l3:l2:l1:s1:s 
          TableScan
            alias: s
            Reduce Output Operator
              key expressions:
                    expr: s_nationkey
                    type: int
              sort order: +
              Map-reduce partition columns:
                    expr: s_nationkey
                    type: int
              tag: 1
              value expressions:
                    expr: s_suppkey
                    type: int
      Reduce Operator Tree:
        Join Operator
          condition map:
               Inner Join 0 to 1
          condition expressions:
            0 {VALUE._col1}
            1 {VALUE._col0}
          handleSkewJoin: false
          outputColumnNames: _col1, _col6
          Select Operator
            expressions:
                  expr: _col6
                  type: int
                  expr: _col1
                  type: string
            outputColumnNames: _col0, _col1
            File Output Operator
              compressed: false
              GlobalTableId: 0
              table:
                  input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                  output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat

  Stage: Stage-8
    Map Reduce
      Alias -> Map Operator Tree:
        $INTNAME 
            Reduce Output Operator
              key expressions:
                    expr: _col0
                    type: int
              sort order: +
              Map-reduce partition columns:
                    expr: _col0
                    type: int
              tag: 0
              value expressions:
                    expr: _col1
                    type: string
        profit:l3:l2:l1:l 
          TableScan
            alias: l
            Reduce Output Operator
              key expressions:
                    expr: l_suppkey
                    type: int
              sort order: +
              Map-reduce partition columns:
                    expr: l_suppkey
                    type: int
              tag: 1
              value expressions:
                    expr: l_orderkey
                    type: int
                    expr: l_partkey
                    type: int
                    expr: l_suppkey
                    type: int
                    expr: l_quantity
                    type: double
                    expr: l_extendedprice
                    type: double
                    expr: l_discount
                    type: double
      Reduce Operator Tree:
        Join Operator
          condition map:
               Inner Join 0 to 1
          condition expressions:
            0 {VALUE._col1}
            1 {VALUE._col0} {VALUE._col1} {VALUE._col2} {VALUE._col4} {VALUE._col5} {VALUE._col6}
          handleSkewJoin: false
          outputColumnNames: _col1, _col2, _col3, _col4, _col6, _col7, _col8
          Select Operator
            expressions:
                  expr: _col4
                  type: int
                  expr: _col7
                  type: double
                  expr: _col8
                  type: double
                  expr: _col6
                  type: double
                  expr: _col3
                  type: int
                  expr: _col2
                  type: int
                  expr: _col1
                  type: string
            outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6
            File Output Operator
              compressed: false
              GlobalTableId: 0
              table:
                  input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                  output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat

  Stage: Stage-1
    Map Reduce
      Alias -> Map Operator Tree:
        $INTNAME 
            Reduce Output Operator
              key expressions:
                    expr: _col0
                    type: int
                    expr: _col4
                    type: int
              sort order: ++
              Map-reduce partition columns:
                    expr: _col0
                    type: int
                    expr: _col4
                    type: int
              tag: 1
              value expressions:
                    expr: _col1
                    type: double
                    expr: _col2
                    type: double
                    expr: _col3
                    type: double
                    expr: _col4
                    type: int
                    expr: _col5
                    type: int
                    expr: _col6
                    type: string
        profit:l3:l2:ps 
          TableScan
            alias: ps
            Reduce Output Operator
              key expressions:
                    expr: ps_suppkey
                    type: int
                    expr: ps_partkey
                    type: int
              sort order: ++
              Map-reduce partition columns:
                    expr: ps_suppkey
                    type: int
                    expr: ps_partkey
                    type: int
              tag: 0
              value expressions:
                    expr: ps_supplycost
                    type: double
      Reduce Operator Tree:
        Join Operator
          condition map:
               Inner Join 0 to 1
          condition expressions:
            0 {VALUE._col3}
            1 {VALUE._col1} {VALUE._col2} {VALUE._col3} {VALUE._col4} {VALUE._col5} {VALUE._col6}
          handleSkewJoin: false
          outputColumnNames: _col3, _col8, _col9, _col10, _col11, _col12, _col13
          Select Operator
            expressions:
                  expr: _col8
                  type: double
                  expr: _col9
                  type: double
                  expr: _col10
                  type: double
                  expr: _col11
                  type: int
                  expr: _col12
                  type: int
                  expr: _col13
                  type: string
                  expr: _col3
                  type: double
            outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6
            File Output Operator
              compressed: false
              GlobalTableId: 0
              table:
                  input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                  output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat

  Stage: Stage-2
    Map Reduce
      Alias -> Map Operator Tree:
        $INTNAME 
            Reduce Output Operator
              key expressions:
                    expr: _col3
                    type: int
              sort order: +
              Map-reduce partition columns:
                    expr: _col3
                    type: int
              tag: 1
              value expressions:
                    expr: _col0
                    type: double
                    expr: _col1
                    type: double
                    expr: _col2
                    type: double
                    expr: _col4
                    type: int
                    expr: _col5
                    type: string
                    expr: _col6
                    type: double
        profit:l3:p 
          TableScan
            alias: p
            Filter Operator
              predicate:
                  expr: (p_name like '%green%')
                  type: boolean
              Filter Operator
                predicate:
                    expr: (p_name like '%green%')
                    type: boolean
                Reduce Output Operator
                  key expressions:
                        expr: p_partkey
                        type: int
                  sort order: +
                  Map-reduce partition columns:
                        expr: p_partkey
                        type: int
                  tag: 0
      Reduce Operator Tree:
        Join Operator
          condition map:
               Inner Join 0 to 1
          condition expressions:
            0 
            1 {VALUE._col0} {VALUE._col1} {VALUE._col2} {VALUE._col4} {VALUE._col5} {VALUE._col6}
          handleSkewJoin: false
          outputColumnNames: _col11, _col12, _col13, _col15, _col16, _col17
          Select Operator
            expressions:
                  expr: _col11
                  type: double
                  expr: _col12
                  type: double
                  expr: _col13
                  type: double
                  expr: _col15
                  type: int
                  expr: _col16
                  type: string
                  expr: _col17
                  type: double
            outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5
            File Output Operator
              compressed: false
              GlobalTableId: 0
              table:
                  input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                  output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat

  Stage: Stage-3
    Map Reduce
      Alias -> Map Operator Tree:
        $INTNAME 
            Reduce Output Operator
              key expressions:
                    expr: _col3
                    type: int
              sort order: +
              Map-reduce partition columns:
                    expr: _col3
                    type: int
              tag: 1
              value expressions:
                    expr: _col0
                    type: double
                    expr: _col1
                    type: double
                    expr: _col2
                    type: double
                    expr: _col4
                    type: string
                    expr: _col5
                    type: double
        profit:o 
          TableScan
            alias: o
            Reduce Output Operator
              key expressions:
                    expr: o_orderkey
                    type: int
              sort order: +
              Map-reduce partition columns:
                    expr: o_orderkey
                    type: int
              tag: 0
              value expressions:
                    expr: o_orderdate
                    type: string
      Reduce Operator Tree:
        Join Operator
          condition map:
               Inner Join 0 to 1
          condition expressions:
            0 {VALUE._col4}
            1 {VALUE._col0} {VALUE._col1} {VALUE._col2} {VALUE._col4} {VALUE._col5}
          handleSkewJoin: false
          outputColumnNames: _col4, _col11, _col12, _col13, _col15, _col16
          Select Operator
            expressions:
                  expr: _col15
                  type: string
                  expr: year(_col4)
                  type: int
                  expr: ((_col11 * (1 - _col12)) - (_col16 * _col13))
                  type: double
            outputColumnNames: _col0, _col1, _col2
            Select Operator
              expressions:
                    expr: _col0
                    type: string
                    expr: _col1
                    type: int
                    expr: _col2
                    type: double
              outputColumnNames: _col0, _col1, _col2
              Group By Operator
                aggregations:
                      expr: sum(_col2)
                bucketGroup: false
                keys:
                      expr: _col0
                      type: string
                      expr: _col1
                      type: int
                mode: hash
                outputColumnNames: _col0, _col1, _col2
                File Output Operator
                  compressed: false
                  GlobalTableId: 0
                  table:
                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat

  Stage: Stage-4
    Map Reduce
      Alias -> Map Operator Tree:
        hdfs://poseidon.smn.cs.brown.edu:9000/media/datavol/hive-scratch/hive_2011-11-05_17-22-45_292_5404155009754439778/-mr-10004 
            Reduce Output Operator
              key expressions:
                    expr: _col0
                    type: string
                    expr: _col1
                    type: int
              sort order: ++
              Map-reduce partition columns:
                    expr: _col0
                    type: string
                    expr: _col1
                    type: int
              tag: -1
              value expressions:
                    expr: _col2
                    type: double
      Reduce Operator Tree:
        Group By Operator
          aggregations:
                expr: sum(VALUE._col0)
          bucketGroup: false
          keys:
                expr: KEY._col0
                type: string
                expr: KEY._col1
                type: int
          mode: mergepartial
          outputColumnNames: _col0, _col1, _col2
          Select Operator
            expressions:
                  expr: _col0
                  type: string
                  expr: _col1
                  type: int
                  expr: _col2
                  type: double
            outputColumnNames: _col0, _col1, _col2
            File Output Operator
              compressed: false
              GlobalTableId: 0
              table:
                  input format: org.apache.hadoop.mapred.SequenceFileInputFormat
                  output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat

  Stage: Stage-5
    Map Reduce
      Alias -> Map Operator Tree:
        hdfs://poseidon.smn.cs.brown.edu:9000/media/datavol/hive-scratch/hive_2011-11-05_17-22-45_292_5404155009754439778/-mr-10005 
            Reduce Output Operator
              key expressions:
                    expr: _col0
                    type: string
                    expr: _col1
                    type: int
              sort order: +-
              tag: -1
              value expressions:
                    expr: _col0
                    type: string
                    expr: _col1
                    type: int
                    expr: _col2
                    type: double
      Reduce Operator Tree:
        Extract
          File Output Operator
            compressed: false
            GlobalTableId: 1
            table:
                input format: org.apache.hadoop.mapred.TextInputFormat
                output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
                name: default.q9_product_type_profit

  Stage: Stage-0
    Move Operator
      tables:
          replace: true
          table:
              input format: org.apache.hadoop.mapred.TextInputFormat
              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
              name: default.q9_product_type_profit

  Stage: Stage-6
    Stats-Aggr Operator


hadoop fs -mkdir /ssb/customer; hadoop fs -copyFromLocal ~/samba/customer.tbl /ssb/customer/
hadoop fs -mkdir /ssb/lineorder; hadoop fs -copyFromLocal ~/samba/lineorder.tbl /ssb/lineorder/
hadoop fs -mkdir /ssb/date; hadoop fs -copyFromLocal ~/samba/date.tbl /ssb/date/
hadoop fs -mkdir /ssb/part; hadoop fs -copyFromLocal ~/samba/part.tbl /ssb/part/
hadoop fs -mkdir /ssb/supplier; hadoop fs -copyFromLocal ~/samba/supplier.tbl /ssb/supplier/

create external table part (P_PARTKEY INT, P_NAME STRING, P_MFGR STRING, P_CATEGORY STRING, P_BRAND STRING, P_COLOR STRING, P_TYPE STRING, P_SIZE INT, P_CONTAINER STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY '|' STORED AS TEXTFILE LOCATION '/ssb/part';
create external table lineorder (lo_orderkey INT, lo_linenumber INT, lo_custkey INT, lo_partkey INT, lo_suppkey INT, lo_orderdate INT, lo_orderpriority STRING, lo_shippriority STRING, lo_quantity INT, lo_extendedprice BIGINT, lo_ordertotalprice INT, lo_discount INT, lo_revenue BIGINT, lo_supplycost INT, lo_tax INT, lo_commitdate INT, lo_shipmode STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY '|' STORED AS TEXTFILE LOCATION '/ssb/lineorder';
create external table supplier (s_suppkey INT, s_name STRING, s_address STRING, s_city STRING, s_nation STRING, s_region STRING, s_phone STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY '|' STORED AS TEXTFILE LOCATION '/ssb/supplier';
create external table customer (c_custkey INT, c_name STRING, c_address STRING, c_city STRING, c_nation STRING, c_region STRING, c_phone STRING, c_mktsegment STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY '|' STORED AS TEXTFILE LOCATION '/ssb/customer';
create external table dwdate (d_datekey INT, d_date STRING, d_dayofweek STRING, d_month STRING, d_year INT, d_yearmonthnum INT, d_yearmonth STRING, d_daynuminweek INT, d_daynuminmonth INT, d_daynuminyear INT, d_monthnuminyear INT, d_weeknuminyear INT, d_sellingseason STRING, d_lastdayinweekfl STRING, d_lastdayinmonthfl STRING, d_holidayfl STRING, d_weekdayfl STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY '|' STORED AS TEXTFILE LOCATION '/ssb/date';



create table dwdate_in (d_datekey INT, d_date STRING, d_dayofweek STRING, d_month STRING, d_year INT, d_yearmonthnum INT, d_yearmonth STRING, d_daynuminweek INT, d_daynuminmonth INT, d_daynuminyear INT, d_monthnuminyear INT, d_weeknuminyear INT, d_sellingseason STRING, d_lastdayinweekfl STRING, d_lastdayinmonthfl STRING, d_holidayfl STRING, d_weekdayfl STRING);
insert overwrite table dwdate_in select * from dwdate order by d_datekey;


create table lineorder_in (lo_orderkey INT, lo_linenumber INT, lo_custkey INT, lo_partkey INT, lo_suppkey INT, lo_orderdate INT, lo_orderpriority STRING, lo_shippriority STRING, lo_quantity INT, lo_extendedprice BIGINT, lo_ordertotalprice INT, lo_discount INT, lo_revenue BIGINT, lo_supplycost INT, lo_tax INT, lo_commitdate INT, lo_shipmode STRING);

insert overwrite table lineorder_in select * from lineorder order by lo_orderkey;

create table lineorder_in_od (lo_orderkey INT, lo_linenumber INT, lo_custkey INT, lo_partkey INT, lo_suppkey INT, lo_orderdate INT, lo_orderpriority STRING, lo_shippriority STRING, lo_quantity INT, lo_extendedprice BIGINT, lo_ordertotalprice INT, lo_discount INT, lo_revenue BIGINT, lo_supplycost INT, lo_tax INT, lo_commitdate INT, lo_shipmode STRING);

insert overwrite table lineorder_in_od select * from lineorder order by lo_orderdate;


-- mod Q1.3
select sum(lo_extendedprice*lo_discount) as revenue from lineorder
where lo_orderdate>=19940210 and lo_orderdate<=19940216
and lo_discount>=5 and lo_discount<=7 and lo_quantity>=26 and lo_quantity<=30;

68.144 sec

select sum(lo_extendedprice*lo_discount) as revenue from lineorder_in
where lo_orderdate>=19940210 and lo_orderdate<=19940216
and lo_discount>=5 and lo_discount<=7 and lo_quantity>=26 and lo_quantity<=30;

select sum(lo_extendedprice*lo_discount) as revenue from lineorder_in_od
where lo_orderdate>=19940210 and lo_orderdate<=19940216
and lo_discount>=5 and lo_discount<=7 and lo_quantity>=26 and lo_quantity<=30;

create table lineorder_in_od (lo_orderkey INT, lo_linenumber INT, lo_custkey INT, lo_partkey INT, lo_suppkey INT, lo_orderdate INT, lo_orderpriority STRING, lo_shippriority STRING, lo_quantity INT, lo_extendedprice BIGINT, lo_ordertotalprice INT, lo_discount INT, lo_revenue BIGINT, lo_supplycost INT, lo_tax INT, lo_commitdate INT, lo_shipmode STRING);

insert overwrite table lineorder_in_od select * from lineorder order by lo_orderdate;


create table lineorder_in_od2 (lo_orderkey INT, lo_linenumber INT, lo_custkey INT, lo_partkey INT, lo_suppkey INT, lo_orderdate INT, lo_orderpriority STRING, lo_shippriority STRING, lo_quantity INT, lo_extendedprice BIGINT, lo_ordertotalprice INT, lo_discount INT, lo_revenue BIGINT, lo_supplycost INT, lo_tax INT, lo_commitdate INT, lo_shipmode STRING) CLUSTERED BY (lo_orderdate) SORTED BY (lo_orderdate) INTO 100 buckets;

select sum(lo_extendedprice*lo_discount) as revenue from lineorder_in_cmp
where lo_orderdate>=19940210 and lo_orderdate<=19940216
and lo_discount>=5 and lo_discount<=7 and lo_quantity>=26 and lo_quantity<=30;


create table lineorder_in_cmp (lo_orderkey INT, lo_linenumber INT, lo_custkey INT, lo_partkey INT, lo_suppkey INT, lo_orderdate INT, lo_orderpriority STRING, lo_shippriority STRING, lo_quantity INT, lo_extendedprice BIGINT, lo_ordertotalprice INT, lo_discount INT, lo_revenue BIGINT, lo_supplycost INT, lo_tax INT, lo_commitdate INT, lo_shipmode STRING) STORED AS RCFILE;


create external table part (P_PARTKEY INT, P_NAME STRING, P_MFGR STRING, P_CATEGORY STRING, P_BRAND STRING, P_COLOR STRING, P_TYPE STRING, P_SIZE INT, P_CONTAINER STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY '|' STORED AS TEXTFILE LOCATION '/ssb/s4/part.tbl';
create external table lineorder (lo_orderkey INT, lo_linenumber INT, lo_custkey INT, lo_partkey INT, lo_suppkey INT, lo_orderdate INT, lo_orderpriority STRING, lo_shippriority STRING, lo_quantity INT, lo_extendedprice BIGINT, lo_ordertotalprice INT, lo_discount INT, lo_revenue BIGINT, lo_supplycost INT, lo_tax INT, lo_commitdate INT, lo_shipmode STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY '|' STORED AS TEXTFILE LOCATION '/ssb/s4/lineorder';
create external table supplier (s_suppkey INT, s_name STRING, s_address STRING, s_city STRING, s_nation STRING, s_region STRING, s_phone STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY '|' STORED AS TEXTFILE LOCATION '/ssb/s4/supplier';
create external table customer (c_custkey INT, c_name STRING, c_address STRING, c_city STRING, c_nation STRING, c_region STRING, c_phone STRING, c_mktsegment STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY '|' STORED AS TEXTFILE LOCATION '/ssb/s4/customer';
create external table dwdate (d_datekey INT, d_date STRING, d_dayofweek STRING, d_month STRING, d_year INT, d_yearmonthnum INT, d_yearmonth STRING, d_daynuminweek INT, d_daynuminmonth INT, d_daynuminyear INT, d_monthnuminyear INT, d_weeknuminyear INT, d_sellingseason STRING, d_lastdayinweekfl STRING, d_lastdayinmonthfl STRING, d_holidayfl STRING, d_weekdayfl STRING) ROW FORMAT DELIMITED FIELDS TERMINATED BY '|' STORED AS TEXTFILE LOCATION '/ssb/s4/date';


select sum(p_size*lo_supplycost) from lineorder join part on (lineorder.lo_partkey=part.p_partkey); 
