package edu.brown.lasvegas.toy;

import java.io.BufferedReader;
import java.io.IOException;
import java.io.InputStream;
import java.io.InputStreamReader;
import java.net.URI;
import java.util.ArrayList;
import java.util.Arrays;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.hdfs.DistributedFileSystem;
import org.apache.hadoop.hdfs.DFSClient.DFSDataInputStream;
import org.apache.log4j.Logger;

/**
 * De-serializes the file generated by {@link IndexTblFile}.
 * So far assumes int key.
 */
public class IndexFileContent {
    private static Logger LOG = Logger.getLogger(IndexFileContent.class);
    private int[] keys;
    private long[] positions;
    private void init (InputStream in) throws IOException {
        LOG.info("reading an index file...");
        ArrayList<Integer> keysList = new ArrayList<Integer>();
        ArrayList<Long> posList = new ArrayList<Long>();
        BufferedReader reader = new BufferedReader(new InputStreamReader(in, "UTF-8"), 1 << 16);
        int prevKey = Integer.MIN_VALUE;
        long prevPos = 0;
        for (String line = reader.readLine(); line != null; line = reader.readLine()) {
            int space = line.indexOf(' ');
            int key = Integer.parseInt(line.substring(0, space));
            long pos = Long.parseLong(line.substring(space + 1));
            assert (key > prevKey);
            assert (pos > prevPos);
            keysList.add(key);
            posList.add(pos);
            prevKey = key;
            prevPos = pos;
        }
        assert (keysList.size() == posList.size());
        reader.close();
        keys = new int[keysList.size()];
        positions = new long[posList.size()];
        for (int i = 0; i < keys.length; ++i) {
            keys[i] = keysList.get(i);
            positions[i] = posList.get(i);
        }
        LOG.info("read " + keys.length + " pointers.");
    }
    
    public IndexFileContent (InputStream in) throws IOException {
        init (in);
    }
    public IndexFileContent (String uri) throws IOException {
        LOG.info("reading index file in DFS..");
        Configuration conf = new Configuration();
        DistributedFileSystem hdfs = (DistributedFileSystem) DistributedFileSystem.
            get(URI.create(uri), conf);
        Path path = new Path(uri);
        DFSDataInputStream in = (DFSDataInputStream) hdfs.open(path);
        init(in);
    }

    /**
     * Tells where to start from for given key.
     * The position might be before the exact tuple because
     * the index is a sparse index.
     * @return the location to start reading.
     */
    public long getStartPosition (int key) {
        int arrayPos = Arrays.binarySearch(keys, key);
        if (arrayPos < 0) {
            // non-exact match. start from previous one
            arrayPos = (-arrayPos) - 1; // this "-1" is binarySearch's design
            arrayPos -= 1; // this -1 means "previous one" 
            if (arrayPos == -1) {
                arrayPos = 0;
            }
        }
        if (arrayPos >= positions.length) {
            arrayPos = positions.length - 1;
        }
        return positions[arrayPos];
    }
    
    /** for debugging. */
    public void dump (Logger logger) {
        logger.info(keys.length + " pointers in the index");
        for (int i = 0; i < keys.length; ++i) {
            logger.info(keys[i] + "->" + positions[i]);
        }
    }
    
    // just for testing
    public static void main(String[] args) throws Exception {
        IndexFileContent c = new IndexFileContent("hdfs://poseidon.smn.cs.brown.edu:9000/ssb/s4/part_pk.tbl.idx");
        // c.dump(LOG);
        LOG.info("0->" + c.getStartPosition(0));
        LOG.info("1->" + c.getStartPosition(1));
        LOG.info("2321->" + c.getStartPosition(2321));
        LOG.info("48621->" + c.getStartPosition(48621));
        LOG.info("148621->" + c.getStartPosition(148621));
        LOG.info("14862100->" + c.getStartPosition(14862100));
    }
}
